# The Smol Training Playbook 한국어 번역

Hugging Face에서 공개한 "The Smol Training Playbook: The Secrets to Building World-Class LLMs"의 한국어 번역본입니다.

SmolLM3는 11조(11T) 개의 토큰으로 훈련된 3B(30억) 파라미터 다국어 추론 모델이며, 이 문서는 해당 모델의 훈련 과정을 상세히 담은 약 400페이지 분량의 가이드입니다.

## 이런 분들에게 추천합니다

- LLM 사전학습을 A부터 Z까지 체계적으로 이해하고 싶은 분
- 사전학습을 시작하고 싶은데 어디서부터 손대야 할지 모르겠는 분
- 데이터는 어떻게 구성하고, 아키텍처는 어떻게 결정하고, 토크나이저는 어떻게 선택해야 하는지 궁금한 분
- 옵티마이저, 학습률, 배치 크기 같은 하이퍼파라미터 선택 기준이 궁금한 분
- 대규모 훈련 인프라(GPU 클러스터, 병렬화 전략, 통신 최적화 등)에 관심 있는 분
- GPU 메모리 계층, NVLink, PCIe 같은 하드웨어 개념을 제대로 이해하고 싶은 분
- 논문만으로는 알 수 없는 실제 훈련 현장의 실패와 디버깅 경험이 궁금한 분
- 체크포인트 관리, 모니터링, 평가 자동화 등 프로덕션 레벨의 운영 노하우가 필요한 분
- 논문이나 블로그에 파편화되어 있는 지식들을 하나로 연결하고 싶은 분

## 목차

### 1. 처음부터 학습이 필요한지 판단하기
- 적합한 학습 프레임워크 선택
- 신뢰할 수 있는 평가 방법 선정

### 2. 모델 아키텍처 탐구
- 어텐션: MHA, GQA, MQA, MLA
- RoPE, NoPE, IntraDoc Masking, SWA 비교
- MoE 및 하이브리드 모델 개요
- 토크나이저 이해
- 옵티마이저 및 하이퍼파라미터 튜닝

### 3. 데이터 프로세스 마스터하기
- 스케일링 법칙의 역사 학습
- 데이터셋 큐레이션 및 효과적인 혼합
- 성능 향상을 위한 학습 데이터셋 구축

### 4. 대규모 학습 및 디버깅
- SmolLM3의 처리량 감소 문제 등 실제 이슈에서 배우기
- 대규모 학습 시 예상치 못한 문제 해결

### 5. 2025년의 포스트 트레이닝: 모델을 유용하게 만들기
- 지도 학습 미세조정(SFT) 베이스라인 설정
- SFT, DPO, GRPO 기법 학습
- RL 적용 시점과 방법 파악
- GRPO를 활용한 SmolLM3 수학 성능 개선

### 6. 인프라 이해하기
- GPU 클러스터의 실제 작동 원리 이해
- CPU, GPU, 노드, 스토리지 간 통신 패턴 학습
- 성능 병목 현상 식별 및 해결

## 주요 내용

### 사전학습의 전 과정을 다룹니다

아키텍처 설계부터 데이터 큐레이션, 옵티마이저와 학습률 선택, 그리고 하드웨어 인프라까지. 사전학습이 정말 필요한가, 필요하다면 무엇이 갖춰져야 하는가에 대한 고민부터 시작합니다.

### 실패와 재시작의 경험을 솔직하게 공유합니다

유망해 보였던 소규모 ablation 실험이 대규모에서는 왜 적용되지 않는지, 1조(1T) 토큰 훈련 후 왜 처음부터 다시 시작해야 했는지를 보여줍니다. 강력한 영어 성능을 유지하면서 다국어 지원, 수학, 코드라는 서로 경쟁하는 목표들을 어떻게 균형 잡았는지도 배울 수 있습니다.

### 인프라 구축의 실무적 고민이 담겨 있습니다

GPU 레이아웃, CPU/GPU/노드/스토리지 간의 통신 패턴 등 인프라를 어떻게 구축해야 하는가에 대한 깊은 고민들이 녹아 있습니다.

## 인상 깊었던 조언들

> "많은 훈련 프로젝트가 실패하는 이유는 잘못된 하이퍼파라미터나 버그 있는 코드 때문이 아니라, 아무도 필요로 하지 않는 모델을 훈련하기로 결정했기 때문이다."

> "아무리 작아도 모든 변경을 테스트하세요. 겉보기에 무해한 라이브러리 업그레이드나 '단 두 줄만 바꾼' 커밋의 영향을 과소평가하지 마세요."

## 링크

- 번역본: https://wikidocs.net/318788
- 원문: https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook

## 감사의 말

이 귀중한 자료를 공개해 주신 Hugging Face 팀에 감사드립니다.